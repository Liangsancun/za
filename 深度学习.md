### 预处理+特征 ###

1.数据向量化

神经网络所有的输入和label目标都是浮点数张量（特定下，是整数张量）

* 文本向量化
  * 文本分割为字符（中文一个字，英文一个字母），将每个字符转换为一个向量
  * 文本分割为单词（中文一个词，英文一个单词），将每个单词转换为一个向量
  * 提取单词或字符的n-gram，将每个n-gram转换为一个向量。

2.值标准化

归一化：手写数字分类

标准化：

```



```

3.处理缺失值

```
空值：pandas中是""
缺失值：df中是NAN(数字，eg:年龄），NaT（日期类型datetime，eg:生日 1995-2-1），None(字符串，eg:性别）
```

3.1判断缺失值

```
df.isnull()返回true／false的DataFrame
df.isnull().any()返回Series,显示哪些列有缺失值(该列名下，其值为true)
df.isnull().sum()返回Series，显示每个列各有多少缺失值
df.info()看每个列有多少个非缺失值
```

3.2处理缺失值

> * 对缺失值较多的特征，删掉该特征（列）
>
> ```python
> #删除含有缺失值的行或列
> df.dropna()#删除包含缺失值的特征
> DataFrame.dropna(axis=0,how='any',thresh=None,subset=None,inplace=Flase)
> '''
> axis=0:对列操作，将该特征舍弃，=1时，舍弃缺失值所在的行
> how:默认'any'，只有一行/列存在缺失值，就将该行/列丢弃，'all'时，一行/列所有的值都是缺失值时才丢弃
> thresh：int型，eg:thresh=3，该行/列至少出现了3个才将其丢弃
> inplace:默认False，True时为直接对数据进行修改
> '''
> 
> ```
>
> * 若样本缺失值适中
>
> ```
> 该特征为离散类型，可以将缺失值作为一个新类别
> 该特征为连续类型，可以将其离散后，把离散值作为一个新类别
> ```
>
> 
>
> * 缺失值较少的特征（10%以内）
>
>   * 填充
>
>  ```python
> df.fillna(method='ffill')==df.ffill()
> df.fillna(method='bfill')==df.bfill()
>     DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)
>  '''
>  value:用什么值填充，常量，字典/Series(按列填充)
>  axis:确定行还是列，与method配套出现
>  method: 不可与value同时出现
>  	'ffill'，axis=0时为用该缺失值所在列上面的值填充该缺失值
>  			 axis=1时为用该缺失值所在行前面的值填充该缺失值
>  	'bfill'，axis=0时为用该缺失值所在列下面的值填充该缺失值
>  	         axis=1时为用该缺失值所在行后面的值填充该缺失值
>  
>  inplace=True时：直接在原数据上进行修改
>  
>  '''
>  ```
>
>  
>
>     * 固定值填充，0填充，只要0不是一个有意义的值
>
>    ```
>    df.fillna(0)
>    ```
>
>     * 均值填充（中位数，众数，最大值，最小值）
>
>    ```pyton
>    #将所有的缺失值按所在列的均值进行填充,value可为常数，字典，Series，但只能按列填充
>    df=df.fillna(df.mean())==df.fillna(df.mean(axis=0))
>    ```
>
>     * 上下数据填充
>
>    ```python
>    #用该列中缺失值上面的那个值来填充该缺失值
>    df.fillna(axis=0,method='ffill')
>    #用该行中缺失值前面的那个值来填充该缺失值
>    df.fillna(axis=1,method='ffill')
>    #用该列中缺失值前面的那个值来填充该缺失值
>    df.fillna(axis=0,method='bfill')
>    #用该行中缺失值上面的那个值来填充该缺失值
>    df.fillnae(axis=1,method='bfill')
>    ```
>
>     * 算法拟合填充(==优先使用==)
>
>    ```python
>    #定义price缺失值预测填充函数
>    from sklearn.ensemble import RandomForestRegressor
>    def set_missing(df):
>    	#把已有的数值型特征取出来
>    	temp_df = df[['price','列名1','列名2',..]]
>        #将数据分为该特征缺失和该特征存在的两部分
>        know=temp_df[temp_df.price.notnull()].values
>        unkown=temp_df[temp_df.price.isnull()].values
>        
>        #X为特征属性值
>        X=know[:,1:]
>        #以price列为y
>        y=know[:,0]
>        
>        #
>      rfr=RandomForestRegression(random_state=0,n_estimators=2000,n_jobs=-1)
>        rfr.fit(X,y)
>        
>        #用得到的预测结果填补原缺失值
>        predicted=rfr.predict(unknow[:,1:])
>        
>        df.loc[(df.price.isnull()),'price']=predicted
>        return df,rfr
>    
>    new_df,rfr=set_missing(df)
>    ```
>
>   * 替换
>
> * 若train中有缺失值，test中无缺失值，可用条件均值/条件中值替换成该缺失值
>
> ```
> 条件均值：该label下的缺失值所属特征的均值
> ```
>
> * 若train和test中均有大量缺失值时，可以将该特征是否有缺失值作为一种特征
>
> ```
> 对于该特征，缺失值变为0，非缺失值为1
> ```
>
> 
>
> 

4.特征工程

选择什么特征给模型学习

### batch_size,epoch,iter ###

> 每batch_size个样本后，进行一次梯度下降，为义词iterations（迭代）
>
> > * 遍历全部数据算一次损失函数，批梯度下降（batch gradient decent）计算开销大，计算速度慢，不支持在在线学习
> > * 一个数据算一次损失函数，随机梯度下降（stochastic gradient descent），速度快，收敛性能不太好，不收敛，在最优点晃来晃去，无法到达优点
> > * mini_batch即batch_size，一般为2的幂(8~128)，便于GPU上的内存分配
>
> epochs为训练多少个完整样本，多少轮
> $$
> one epoch = iterations \times batch\_size
> $$
> 在LSTM中我们还会遇到一个seq_length,其实 
> batch_size = num_steps * seq_length



欠拟合和过拟合

![1561344090844](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1561344090844.png)

![1561344104645](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1561344104645.png)

欠拟合和过拟合的表现

> * 欠拟合：训练集上准确率不高，测试集上准确率也不高【语料偏少，特征偏多】
> * 过拟合：训练集上准确率特别高，在测试集上的准确率不高

防止过拟合

> * 最优：数据集扩增：让训练集中的噪音数据比重占比少
>
> * 次优：对模型允许存储的信息加以约束，如果一个网络只能记住几个模式，就只让其集中学习最重要的模式，降低模型复杂度
>
>   * 减少网络容量：==减小参数个数（【容量】eg:层数，隐藏单元数），来降低模型复杂度==
>
>   * 权重正则化(weight regularization)，==通过强制让模型权重取较小的值（让模型权重变小）来降低模型复杂度==，奥卡姆剃刀（如无需要，勿增实体，若有两个解释，选择最简单，假设更少的那个）
>
>     * 范数，$x是一个向量，它的Lp范数$:
>      $$
>      \|x\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}
>      $$
>
>     * 正则化方法 目标函数后加一个系数的惩罚项
>      $$
>      \overline{J}(w, b)=J(w, b)+\frac{\lambda}{2 m} \Omega(w)\\
>       \frac{\lambda}{2 m}是常数，m为样本数，\lambda是超参数，用于控制正则化程度
>      $$
>
>       * L1正则化时，让原目标函数加上了**所有特征系数绝对值的和**，对应惩罚项为：
>      $$
>         \Omega(w)=\|w\|_{1}=\sum_{i}\left|w_{i}\right|
>      $$
>
>       * L2正则化时，让原目标函数加上了**所有特征系数的平方和**，对应惩罚项为：
>      $$
>         \Omega(w)=\|w\|_{2}^{2}=\sum_{i} w_{i}^{2}
>      $$
>
>     * L1正则项，让特征变得稀疏（特征权重在0附近，有的>0，有的<0，有的=0），==更适合模型选择==（额外的惩罚项使得：若w1为正，则每次更新还会额外减去一个常数，若w1为负，每次更新还会额外加上一个常数，很容易产生特征的系数为0的情况）
>      $$
>       假设只有两个特征w_1,w_2，\eta 为学习率，L1正则化的目标函数为：\\
>       \overline{J}=J+\frac{\eta \lambda}{2 m}\left(\left|w_{1}\right|+\left|w_{2}\right|\right)
>      $$
>
>      $$
>       w_1更新时：朝着斜率相反的方向，使得目标函数值更小,sign(w)值是正时为1，是负时为1\\
>       \begin{array}{c}{w_{1} =w_{1}-\eta \frac{\partial\overline{J}}{\partial w_{1}}} \\ {=w_{1}-\eta（\frac{\partial J}{\partial w_{1}}+\frac{\eta \lambda}{2 m}\operatorname{sign}\left(w_{1}\right)}） \\ {=w_{1}-\frac{\eta \partial J}{\partial w_{1}}}-\frac{\eta \lambda}{2 m} \operatorname{sign}\left(w_{1}\right)\end{array}
>      $$
>
>       
>
>     * L2正则化，特征权重缩小到0附近，但是没有=0的，==更适合防止过拟合==（额外的惩罚项使得：每次更新，对特征的权重进行一个比例的缩小）
>      $$
>       假设只有两个特征w_1,w_2，\eta 为学习率，L2正则化的目标函数为：\\
>       \overline{J}=J+\frac{\eta \lambda}{2 m}\left(w_{1}^{2}+w_{2}^{2}\right)
>      $$
>       ​	
>      $$
>       w_1更新时：朝着斜率相反的方向，使得目标函数值更小\\
>       \begin{array}{c}{w_{1} =w_{1}-\eta \frac{\partial \overline{J}}{\partial w_{1}}} \\ {=w_{1}-\eta（\frac{\partial J}{\partial w_{1}}+\frac{\eta \lambda}{2 m}(2w_1)}） \\ {=w_{1}-\frac{\eta \partial J}{\partial w_{1}}}-\frac{\eta \lambda}{m} \left(w_{1}\right)\end{array}
>      $$
>      
>
>   * dropout正则化 （最有效，最常用）每个样本的随机删除一些隐藏的神经元，同时保证输入层和输出层的神经元不变
>
>     * 银行，柜员不停换人，避免互相合作进行欺诈；在每个样本中随机删除不同的部分神经元(为0)，可以组织它们的阴谋，降低过拟合；在层的输出值中引入噪声，打破不显著的偶然模式（阴谋）
>
>     * 对某一层使用dropout，在训练过程中，随机将该层的一些输出特征舍弃（设置为0）
>
>     * dropout比率（dropout rate）是被设为0的特征所占的比例，通常在0.2~0.5
>
>       ```python
>       1. 训练+测试时操作
>       #训练时，舍弃了一半的输出单元
>       train_layer_output *= np.random.randint(0,high=2,size=layer_output.shape)
>       #测试时，因为前面舍弃了一半的单元
>       test_layer_output *= 0.5
>       
>       2. 训练时操作
>       #训练时，舍弃了一半的输出单元
>       train_layer_output *= np.random.randint(0,high=2,size=layer_output.shape)
>       #可让这两个运算同时在训练时进行，而测试时输出不变
>       train_layer_output /= 0.5
>       
>       3. keras实现
>       #在想要引入dropout的层后加上
>       model.add(layers.Dropout(0.5))#rate=0.5
>       ```
>
>       
>
> * early stopping：当accuracy不高时，就停止训练
>
> * 挑选合适模型

### one-hot编码/分类编码(categorical encoding) ###

```python
def to_one_hot(labels,dimension):#labels为总样本，dimension为样本的类别数
	results=np.zeros(len(labels),dimension)
	for i,label in enumerate(labels):
		results[i,label]=1.0#label可为标量或向量，元素时数字代表类别，eg：5个类别 第2个label=[3，4],则在results[2]
	return results
```



> * 层数/隐藏节点个数，越多，计算代价越大，训练时间越长，此外，还可能由欠拟合过渡到过拟合
>
> * 单词序列可以编码为二进制向量（one-hot），但是结果相当于set()，体现不出来序列的顺序和重复的词，和意思相同的词
>
> * 不同特征有不同取值范围值，要进行预处理，应对每个特征单独进行标准化$\frac{(x-\overline {x})}{x.std}$
>
>   * ```python
>    mean=train_data.mean(axis=0)#求每个特征的平均值,axis=0为按列计算
>    train_data -= mean
>    std = train_data.std(axis=0)#求每个特征的标准差
>    train_data /= std
>    ```
>  ```
> 
>  ```
>
> ```
> 
> ```
>
> test_data -= mean
> test_data /= std
>
> ```python
>
> 
>
> * 带relu激活的Dense堆叠，可解决很多问题（eg:情感分类）
>
> * 优化器 'rmsprop'万金油
>
> * 当数据量较少时，可使用较少隐藏层（一到两个），避免严重的过拟合。
>
> * 当数据量较少时，可以采用k-折验证的方法，可靠的评估模型
>
>   * ```python
>     #k折验证
>     import numpy as np
>     k=4
>     num_val_samples=len(train_data)//k
>     num_epochs=100
>     all_scores=[]
>     ```
> ```
> 
> for i in range(k):#0-（k-1)
> print('processing fold',i)
> #验证集
> val_data=train_data[i*num_val_samples:(i+1)*num_val_samples]
> val_targets=train_targets[i*num_val_samples:(i+1)*num_val_samples]
> #在列上进行拼接，不同的样本群拼接起来
> partial_train_data = np.concatenate((train_data[:i*num_val_samples],train_data[(i+1)*num_val_samples:]),axis=0)
> partial_train_targets = np.concatenate((train_targets[:i*num_val_samples],train_targets[(i+1)*num_val_samples:]),axis=0)
> 
> #构建模型（已编译）
> model=build_model()#见回归问题
> #当有验证集时，返回的History对象的history元素的keys为['val_loss', 'val_mean_absolute_error', 'loss', 'mean_absolute_error']
> model.fit(partial_train_data,partial_train_targets,epochs=num_epochs,batch_size=1,verbose=0)
> #返回均方误差和平均绝对误差
> val_mse,val_mae = model.evaluate(val_data,val_targets,verbose=0)
> all_scores.append(val_mae)
> 
> ```
>
> 
>
> * 对于二分类问题
>
> ```python
> #对于二分类,最后一层的情况,只有一个节点（输出0D标量，是个0~1的值）
> model.add(layers.Dense(1,activation='sigmoid'))
> #二元交叉熵或均方误差'mean_squared_error'
> model.compile(loss='binary_crossentropy')
> ```
> 
> * 对于多分类问题
> 
> ​```python
> #对于n分类
> #隐藏层节点的个数，相对于n，不能太小，以免在网络中造成信息瓶颈（将大量信息压缩到维度很小的中间空间）导致模型的效果不佳
> #最后一层为n的Dense层,激活函数为softmax，此时一个样本属于所有类别的概率为1
> model.add(layers.Dense(n,activation='softmax'))
> #分类交叉熵，网络输出的概率分布与目标的真实分布之间的距离最小化
> model.compile(loss='categorical_crossentropy')
> 
> #处理标签有两种方法
>   1. 分类编码（ont-hot编码）,loss='categorical_crossentropy'
> from keras.utils.np_utils import to_categorical
> one_hot_train_labels = to_categorical(train_labels)
>   2. 将标签编码为整数，loss='sparse_categorical_crossentropy'
> from sklearn import preprocessing
> le=preprocessing.LabelEncoder()
> le.fit_transform(array-like)
> 	le.fit(array-like)
> 	le.transform(array-like)
> ```
>
> * 回归问题
>
> ```python
> '''
> 损失函数为均方误差:loss='mse'
> 评估指标为平均绝对误差：metrics=['mae']
> model.fit(validation_data=(val_data,val_targets))返回History.history.keys()=['val_loss', 'val_mean_absolute_error', 'loss', 'mean_absolute_error']
> 
> '''
> def build_model():
> #定义模型
> model = models.Sequential()
> model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],)))
> model.add(layers.Dense(64,activation='relu'))
> model.add(layers.Dense(1))#最后一层只要一个单元，没有激活，是一个线性层，标量回归
> #编译模型 loss 均方误差 mse ，指标：平均绝对误差 mean absolute error，预测值和目标值之差的绝对值
> model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])
> return model
> ```
>
> 

### 序列模型<code>线性的层次堆叠</code> ###

```python
from keras import models
from keras import layers
model=models.Sequential()

#添加层
#layers.Dense为全连接层，16为该层隐藏层单元也即输出的一个样本的张量形式，input_shape为输入的一个样本的张量形式
model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))#x_train=(10000,)输入：一个样本为10000维的1D张量
model.add(layers.Dense(16,activation='relu'))#当不填input_shape时，会直接根据上一层得到的数据进行调整
model.add(layers.Dense(1,activation='sigmoid'))#y_train=(1,)输出：一个样本为1维的标量（0D张量）
#编译模型，优化器，损失函数，指标
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])

#训练模型，返回了一个History对象，该对象有一个成员history，是一个字典，其中记录了每一轮训练的指标结果，此时将训练数据分出来一部分用于验证集validation_data
history=model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))
history_dict=history.history#['acc','val_acc','loss','val_loss']
#画图,找出合适的训练轮数epochs，隐藏层层数，每层隐藏层的隐藏节点个数
import matplotlib.pyplot as plt
%matplotlib inline
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1,len(loss_values)+1)
plt.plot(epochs,loss_values,'bo',label='Training loss')
plt.plot(epochs,val_loss_values,'b',label='Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and validation loss')
plt.legend(loc='best')
plt.show()

plt.clf()#清空图像
acc=history_dict['acc']
val_acc=history_dict['val_acc']
plt.plot(epochs,acc,'bo',label='Training acc')
plt.plot(epochs,val_acc,'b',label='Validation acc')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc='best')
plt.show()

# 重新构建网络（因为刚才的模型已经过拟合或者其他问题，不合适了），拿所有的训赖你数据进行训练

#定义模型
model=models.Sequential()
model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))
model.add(layers.Dense(16,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))
#编译模型
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])
#训练模型
model.fit(x_train,y_train,epochs=4,batch_size=512)#从图中可看出应选择训练4轮
#用测试集评估模型
results=model.evaluate(x_test,y_test)#返回损失值和选定的指标值（例如，精度accuracy）[0.29937357312202456, 0.88244]
#预测
model.predict(x_test) 1D张量，维数是预测样本数
```





> 方差： 各数据偏离平均值的距离平方和的平均数 
>
> 总体方差：
> $$
> s^2=\frac{1}{n}\sum_{i=1}^n{(x_i-\overline{x})^2}​
> $$
> 样本方差：
> $$
> s^2=\frac{1}{n-1}\sum_{i=1}^n{(x_i-\overline{x})^2}​
> $$
> 标准差：方差开平方，也即均方差
>
> $s=\sqrt{s^2}$
>
> 总体标准差：
> $$
> s=\sqrt{\frac{1}{n}\sum_{i=1}^n{(x_i-\overline{x})^2}}
> $$
> 样本标准差：
> $$
> s=\frac{1}{n-1}\sum_{i=1}^n{(x_i-\overline{x})^2}
> $$
> 
>
> 均方误差： 各数据偏离真实值的距离平方和的平均数 ， 也即误差平方和的平均数 
>
> $MES=\frac{1}{n}\sum_{i=1}^n{(pre_i-y_i)^2}$
>
> 均方根值：均方误差开平方
>
> $\sqrt{MES}$

指标

> * metrics=['mae'] 平均绝对误差 mean absolute error，预测值和目标值之差的绝对值
> * metrics=['acc']准确率

keras中model的fit和evaluate中的verbose

> verbose：日志显示
>
> * fit中
>
>   verbose = 0 为不在标准输出流输出日志信息
>   verbose = 1 为每个epoch输出进度条记录
>   verbose = 2 为每个epoch输出一行记录
>   注意： 默认为 1
>
> * evaluate中
>
>   默认为1
>
>   0 or 1

### 损失函数 ###

> * 交叉熵（crossentropy)
>
>   用于衡量输出的概率分布和目标的真实分布之间的距离，对于输出概率值的模型最合适（通过使得输出的概率分布和目标的真实分布之间的距离最小化）
>
>   对于二分类问题，网络输出为一个概率值（最后一层用sigmoid激活函数，仅包含一个单元），适合使用binary_crossentropy（二元交叉熵损失）
>
> * 均方误差： 各数据偏离真实值的距离平方和的平均数 ， 也即误差平方和的平均数 
>
>   $MES=\frac{1}{n}\sum_{i=1}^n{(pre_i-y_i)^2}$
>
>   * 回归中当作损失函数使用
>
> 

### 激活+损失函数选择 ###

| 问题类型       | 最后一层激活       | 编译时loss               | 编译时指标          |
| -------------- | ------------------ | ------------------------ | ------------------- |
| 二分类         | sigmoid            | binary_crossentropy      | accuracy            |
| 多分类，单标签 | softmax(概率和为1) | categorical_crossentropy | accuracy            |
| 多分类，多标签 | sigmoid            | binary_crossentropy      |                     |
| 回归到任意值   | 无                 | mse均方误差              | mae 平均绝对误差    |
| 回归到0~1      | sigmoid            | mse或binary_crossentropy | binary_crossentropy |

### keras使用 ###

```python

model.summary()#打印模型的结构
#训练，batch_size为2个幂次，方便分配内存
model.fit(x_train,y_train,epochs=100,batch_size=32,validation_split=0.2)
model.fit(x_train,y_train,epochs=100,batch_size=32,validation_data=(x_val,y_val))

#分词
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
tokenizer=Tokenizer(num_words=1000)#只考虑前1000个最常见的单词，构建分词器
tokenizer.fit_on_texts(samples)#构建单词索引，即单词用哪个整数表示，samples文档列表,没有0索引，从1开始,samples ：要用以训练的文本列表
word_index=tokenizer.word_index#单词索引,索引从1开始，{'the': 1,'cat': 2,}
sequences=tokenizer.texts_to_sequences(samples)#将字符串转换为整数索引组成的列表 字符串列表--> [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]
data=pad_sequences(texts)#返回array 获取定长的文档向量 texts文档的向量表示,填充序列,让每个文档序列只保留定长的单词量，默认，不足的在最前面补0，多的把前面的舍弃
'''
keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32',
    padding='pre', truncating='pre', value=0.)

sequences：浮点数或整数构成的两层嵌套列表
maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.
dtype：返回的numpy array的数据类型
padding：‘pre’或‘post’，默认'pre',确定当需要补0时，在序列的起始还是结尾补
truncating：‘pre’或‘post’，默认'pre',确定当需要截断序列时，从起始还是结尾截断
value：浮点数，此值将在填充时代替默认的填充值0
'''

#输入(samples，maxlen），输出(samples, maxlen, output_dim=8) 每个单词用8维的1D张量表示
model.add(layers.Embedding(10000,8,input_length=maxlen))
'''
input_dim=10000：共考虑这么多个单词。字典长度,大或等于0的整数，即输入数据最大下标+1(0~9999时，为10000)
output_dim=8：大于0的整数，代表全连接嵌入的维度，每个单词用多少维的1D张量表示
input_length=maxlen:每个样本的维度。当样本为一个文档字符串时，此时为每个文档固定的单词数，eg:每个文本按只保留20个高频率单词（1万个）的单词
'''
#将三维的嵌入张量(samples, maxlen, output_dim=8) 展平成形状(samples,maxlen*8)的二维张量
model.add(layers.Flatten())

#训练时
model.layers[0].set_weight([embedding_matrix])#加载词向量
model.layers[0].trainable=False#冻结Embedding层，不训练词向量

#保存权重
model.save_weight('pre_trained_xx_model.h5')

#加载训练权重
model.load_weights('trained.model.h5')
#评估测试集结果
model.evaluate(x_test,y_test)
```



### 问题： ###

![1561129922291](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1561129922291.png)

![1561127453704](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1561127453704.png)

解决：进入site-package\keras\datasets\imdb.py 262行，如下修改，然后重启(关掉jupyter notebook，再重新启动)

```
- with np.load(path) as f:
+ with np.load(path, allow_pickle=True) as f:
```



