# 处理数据 #

```
array_like：list，numpy_array,DataFrame
baseline：一个基础模型，算法提升的参照物。可以以此为基准来比较对模型的改进是否有效。通常在一些竞赛或项目中，baseline就是指能够顺利完成数据预处理、基础的特征工程、模型建立以及结果输出与评价，然后通过深入进行数据处理、特征提取、模型调参与模型提升或融合，使得baseline可以得到改进。所以这个没有明确的指代，改进后的模型也可以作为后续模型的baseline。
```

[TOC]

### 标准化/归一化：<code>让每个特征重要性一样，消除量纲</code> ###

> eg: 2个人体重差10kg，身高差0.2cm，衡量差别时，体重的差距会把身高的差距掩盖掉。

![1559962577418](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559962577418.png)

1. 归一化：<code>将原始数据某一列数值特征的值缩放到[0,1]</code>
   $$
   x^*=\frac{x-x_{min}}{x_{max}-x_{min}}
   $$
   

   ![1559963024677](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559963024677.png)

2. 标准化：<code>将原始数据的某一列进行缩放，使其服从标准正太分布</code>
   $$
   x^*=\frac{x-\overline{x}}{\sigma} ~~~std=\sigma
   $$
   
3. 区别

   ```
   在不涉及距离度量、协方差度量、数据不符合正太分布时，可以使用归一化方法，如图像处理时，将RGB图像转换为灰度图像后将其值限定在[0, 255]的范围。
   在分类、聚类算法中，需要使用距离来度量相似性的时候，或使用PCA技术进行降维的时候，标准化表现更好。
   归一化改变原始数据的分布；标准化不改变原始数据的分布，在进行特征提取时，消除不同特征的量纲之后，而保留样本在各个维度的分布。
   ```

4. 重点

   ```
   •归一化和标准化的相同点都是对某个特征（column）进行缩放（scaling）而不是对某个样本的特征向量（row）进行缩放。对特征向量进行缩放是毫无意义的（暗坑 1）  比如三列特征：身高、体重、血压。每一条样本（row）就是三个这样的值，对这个 row 无论是进行标准化还是归一化都是好笑的，因为你不能将身高、体重和血压混到一起去！
   •在线性代数中，将一个向量除以向量的长度，也被称为标准化，不过这里的标准化是将向量变为长度为 1 的单位向量，它和我们这里的标准化不是一回事儿，不要搞混哦（暗坑 2）。
   •逻辑回归，如果你不用正则，那么，标准化并不是必须的，如果你用正则，那么标准化是必须的。（暗坑 3）
   
   ```

```python
#标准化,返回numpy数组
from sklearn.preprocessing import StandardScaler
#第一标准化，直接返回标准化后的结果，没有保存特征均值和方差
StandardScaler().fit_transform(array_like)
#第二种标准化，保存有特征均值和方差，可用于标准化其他数据
scaler=StandardScaler().fit(array_like)
scaler.transform(array_like)

StandardScaler(copy=True, with_mean=True, with_std=True)
'''
copy：为Falas时，用归一化的值代替原来的值，可减少内存的使用，但当被被标准化的数据不是np.array或或scipy.sparse CSR matrix, 原来的数据还是被copy而不是被替代

with_mean: 在处理sparse CSR或者 CSC matrices 一定要设置False不然会超内存
'''
          
scaler.get_params(deep=True))#返回设置的参数 
	 {‘with_mean’: True, ‘with_std’: True, ‘copy’: True}
scaler.scale_#缩放比例，同时也是标准差
scaler.mean_#每个特征的均值
scaler.var_#每个特征的方差
scaler.n_sample_seen_#样本数量
```

### 评价指标原则 ###

```
metrics中所有的方法，都是高的值效果>低的值
metrics.accuract_score等以_score结尾的都是值越高越好
metrics.mean_squared_error等以_error结尾的都是值越小越好，所以会返回其负值，'neg_mean_squared_error'，使得最后的结果是值高的更好
mse：’neg_mean_squared_error',metrics.mean_squared_error
mae:’neg_mean_absolute_error',metrics.mean_absolute_error
```



### 分类模型（监督学习）评价指标：了解模型的泛化能力 ###

![1559961371409](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559961371409.png)
$$
TP（True Positive）：预测答案正确\\
FP（False Positive）：错将其他类预测为本类\\
FN（False Negative）：本类标签预测为其他类标\\
$$

0. 混淆矩阵/误差矩阵

   > 呈现多个类别是否有混淆（即一个类别被预测成另一个类别）
   >
   > 行表示实际的类别，列表示预测的类别

   ![1559982865769](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559982865769.png)

   以两个类别为例：

   ![1559983600782](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559983600782.png)

   ```
   T(True)预测正确数目，F(False)预测错误数目
   P(positive)预测为1数目，N(negatibe)预测为0数目
   ```

1. accuracy 准确率：整体预测的准确程度

   ![1559983565629](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559983565629.png)
   $$
   accuracy=\frac{TP+TN}{TP+FP+FN+TN}=\frac{预测正确的样本数(包含正样本和负样本)}{总样本数}
   $$

   ```
   当样本不均衡时，无参考价值
   eg：99个正，1个负，都预测为正，准确率99%
   故，有了recall,f1，precise(有点不太合适，见ROC/AUC中)
   ```

   

2. precise 精准率/查准率：指被分类器判定正例中正样本的比重：对正样本预测的准确成簇

![1559984169829](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559984169829.png)
$$
precise=\frac{TP}{TP+FP}=\frac{预测为正的正样本数目}{预测为正的样本数目}\\
分词时：precise=\frac{正确分词的个数}{分类器分词的个数}
$$


3. recall 召回率/查全率：实际为正的样本中被预测为正样本的概率

   ![1559984572721](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559984572721.png)

$$
recall=\frac{TP}{TP+FN}=\frac{预测为正的正样本数目}{实际为正的样本数目}\\
分词时：recall=\frac{正确分词的个数}{真实分词的个数}
$$



4. f1：precise和recall的调和平均数：同时考虑查准率和召回率

$$
\frac{1}{f1}=\frac{1}{2}\times [\frac{1}{precise}+\frac{1}{recall}]
\\
f1=\frac{2\times precise \times recall}{precise+recall}
$$

5. ROC/AUC   <code>无视样本不均衡</code>
$$
真正率(TPR)=召回率(recall)=灵敏度=\frac{TP}{TP+FN}=正样本的召回率\\
   假正率(FPR)=1-特异率=1-\frac{TN}{FP+TN}=\frac{FP}{FP+TN}=负样本中有多少被当成正样本\\
$$
   不同于准确率，当样本不均衡时，真正率和假正率也不受影响

   * 当底是实际的正样本数或实际的负样本数为底时，样本不均衡不受影响
     * 召回率/真正率
   
   * 当底包含同时包含实际为正和实际为负的样本时，样本不均衡受影响
     * 准确率 accuracy
     * 精准率 precise

   ROC曲线 接收者操作特征曲线（receiver operating characteristic）

   > 

宏平均（Macro-averaging）macro avg

```
指所有类别的每一个统计指标值的算数平均值，也就是宏精确率（Macro-Precision），宏召回率（Macro-Recall），宏F值（Macro-F Score）
```

$$
\begin{array}{c}{\text { macro }_P=\frac{1}{n} \sum_{i=1}^{n} P_{i}} \\ {\text { macro }_r=\frac{1}{n} \sum_{i=1}^{n} R_{i}} \\ {\text { macro }_{F 1}=\frac{2 * P_{\text { macro }} * r_{\text { macro }}}{P_{\text { macro }}+r_{\text { macro }}}}\end{array}
$$

加权平均（weighed-averaging）weighed avg

```
所有类别的每一个统计指标值的加权平均值：即加权精确率，加权召回率，加权F值 
```

使用方法

```python
#获取accuracy,precise,recall,f1,macro avg, weighted avg
classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)

默认返回string,output_dict=True时，返回字典
              precision    recall  f1-score   support

      ok web       0.94      0.96      0.95     15062
    evil net       0.96      0.93      0.95     14376

    accuracy                           0.95     29438
   macro avg       0.95      0.95      0.95     29438
weighted avg       0.95      0.95      0.95     29438
'''
使用：print(classclassification_report(y_true, y_pred, labels=[1,0], target_names=['good','bad'],digit=5))

y_true:真实的标签
y_pred:预测的标签
labels:是标签的数组，默认从小到大，数字从0开始，字母从a开始
target_names:字符串数组，labels所代表的意思，在输出时替代labels中的元素在报告中显示的
sample_weight:数组，不同类型的所占比重，报告中要用到加权平均
digits:浮点数小数占几位,当output_dict=True时，失效

	  {'label 1': {'precision':0.5,
                         'recall':1.0,
                         'f1-score':0.67,
                         'support':1},
             'label 2': { ... },
              ...
       }
       
'''

#获取混淆矩阵
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)
返回二维numpy数组，默认

c=[[14532   530]
 [  965 13411]]
c_ij=实际为i,预测为j
	预测
实
际
'''
使用：confusion_matrix(y_true,y_pred,labels=[1,0])
y_true:真实的标签
y_pred:预测的标签
labels:是标签的数组，默认从小到大，数字从0开始，字母从a开始
'''
画混淆矩阵图
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
def cm_plot(self,y_real,y_pred,labels=None):
    # 参数为实际分类和预测分类
    cm = confusion_matrix(y_real,y_pred)# #返回混淆矩阵，二维numpy数组
    #此时labels采用默认从0开始，从小到大，不然后面画图时比较麻烦
    plt.matshow(cm,cmap=plt.cm.Greens)# 画混淆矩阵图，配色风格使用cm.Greens
    plt.colorbar()# 颜色标签

    dim=cm.shape[0]#因为是方阵，所以长宽相同
    for x in range(dim):
       for y in range(dim):
            plt.annotate(cm[x,y],xy=(x,y),horizontalalignment='center',verticalalignment='center')
                #annotate主要在图形中添加注释# 第一个参数添加注释 数字，各种情况下的数字
                # xy设置箭头尖的坐标
                 #horizontalalignment水平对齐
                #verticalalignment垂直对齐
                
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()


```

### 训练集、验证集、测试集 ###

训练集上训练，验证集上评估，找到最佳参数后，在测试集上测试

为什么要分为三个集？而不是两个？

> 因为信息泄露(information leak)，每次通过模型在验证集上的性能作为反馈信号，进行调节参数，本质上就是对验证集的学习

评估模型注意事项

* 数据代表性（data representativeness）
  * 不能是训练集类别是：0~7，验证集是8~9，要==随机打乱==
* 时间箭头（the arrow of time）
  * 如果要预测未来，不能将数据进行==随机打乱==，会造成时间泄露（temporal leak）。此时应该，验证集和训练集都是一段时间的数据，同时，==测试集上的数据时间要晚于训练集数据==
* 数据冗余（redundancy in your data）
  * 训练集和验证集之间没有交集，同一样本点不应该同时出现在训练集和验证集上

### 回归模型（监督学习）评价指标：了解模型的泛化能力 ###

1. 平均绝对误差（Mean Absolute Error mae）
   $$
   MAE=\frac{1}{n}\sum_{i=1}^n|(pre_i-y_i)|
   $$
   
2. 均方误差（Mean Squared Error mse）
   $$
   MSE=\frac{1}{n}\sum_{i=1}^n{(pre_i-y_i)^2}
   $$
   
3. 均方根误差（Root Mean Squared Error rmse）
   $$
   RMSE=\sqrt{MSE}
   $$
   
4. R^2：拟合优度【0~1】
   $$
   R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}\\
   SST=SSR+SSE\\
   SST(total sum of squares)：平方总和\\
   SSR(regression sum of squares)：回归平方和\\
   SSE(error sum of squares)：残差平方和\\
   $$

```python
metrics中所有的方法，都是高的值效果>低的值
metrics.accuract_score等以_score结尾的都是值越高越好
metrics.mean_squared_error等以_error结尾的都是值越小越好，所以会返回其负值，'neg_mean_squared_error'，使得最后的结果是值高的更好
mse：’neg_mean_squared_error',metrics.mean_squared_error
mae:’neg_mean_absolute_error',metrics.mean_absolute_error

```



### 交叉验证: ###

$$
1. 在数据量不足的情况下，检验模型的泛化能力\\
2. 用于调节参数\\
3. 用于模型选择\\
4. 用于特征选择\\
$$

###  ###

1. 简单交叉验证/留出验证(hold-out validation) ==数据量很大时采用==

```
#随机分成两份，一份做训练集，一份做验证集（eg:70%训练集，30%测试集）
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)
'''
data为特征
labels打的标签
test_size为测试样本占总体样本的比例 0~1
random_state为随机数编号（种子），即当重复试验时，按照该编号，两次可以得到一样的测试样本集，不同的编号对应不同的测试样本集，但为0或不填时，无此效果。
'''
```

2. k-折交叉 ==留出验证的样本量太少，无法保证可靠性==

   ```
   给定数据充足时，将数据随机分为训练集，验证集，测试集，不充足时，采取交叉验证（可重复使用数据）【由于样本数据不多，验证集的划分方式可能会造成验证分数上有很大的方差，不能对模型进行可靠的评估==>k-折交叉验证】
   k常取4 Or 5
   验证：测试模型泛化效果，交叉：重复使用数据
   ```

```python
将数据分成k份，每次k-1份做训练集，1份做验证集，重复做k次实验，每次验证集不同，最后将结果平均
1. 
from sklearn.model_selection import cross_validate
#使用cross_validate而不是cross_val_score因为可以返回多个评价指标
from sklearn.model_selection import KFold#k折
kf=KFold(n_splits=3,shuffle=True,random_state=1)

'''
n_splits：几折
shuffle: 是否随机。shuffle=False时，无随机，两次得到相同；shuffle=True且时random_state=0 or不填时，随机，两次得到不同，shuffle=True且random_state有值（不为0）时，随机，两次得到相同
random_state:当shuffle=True时可用，随机数编号（种子），即当重复试验时，按照该编号，两次可以得到一样的测试样本集，不同的编号对应不同的测试样本集，但为0或不填时，无此效果
'''
2. 
from sklearn.model_selection import StratifiedKFold#分层k折，每折中各类别比例和总数据集相当
skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=1)#同KFold()用法

#配合得出结果
#分类指标
scoring=['accuracy','precision','recall','f1']
#回归指标
scorint=['neg_mean_absolute_error','neg_mean_squared_error']
result=cross_validate(self.model,self.X,self.y,cv=kf,scoring=scoring,n_jobs=2)#返回字典
'''
第一个参数时选择的Model
X：总的data
y:总的labels
cv:决定交叉验证策略。
	int or cross-validation generator
	eg: 5 分层5折 stratified KFold
scoring:选择的评价指标，string or list
n_jobs:计算使用的cpu数，-1意味着使用所有，默认None为1个cpu

'''
accuracy = sum(result['test_accuracy'])/len(result['test_accuracy'])
precision=sum(result['test_precision'])/len(result['test_precision'])
recall = sum(result['test_recall'])/len(result['test_recall'])
f1 = sum(result['test_f1'])/len(result['test_f1'])

for dev_index_arr,val_index_arr in kf.split(data):#k次实验，每次k-1份为训练集，1份为验证集
	pass
for fold_index,(dev_index_arr,val_index_arr) in enumerate(kf.split(data)):
    pass


```

3. 打乱数据的重复k折验证 ==可用数据很少，模型评估要求非常准确==
   * 多次使用K-折验证，每次将数据划分为K个分区前，先把数据打乱
4. 留一法

```
同k-折相同，不过，k值为样本数，用于小样本
```

### 画图 ###

* ![img](https://matplotlib.org/2.0.0/_images/named_colors.png)
* 通用

```python
import matplotlib.pyplot as plt

#设置全局变量
plt.rcParams
plt.rcParams['font.sans-serif']=['SimHei']#正常显示中文
plt.rcParams['axes.unicode_minus']=False # 用来正常显示负号

# 刻度大小
plt.rcParams['axes.labelsize']=16
# 线的粗细
plt.rcParams['lines.linewidth']=17.5

plt.rcParams['xtick.labelsize']=14 # x轴标签数字大小
plt.rcParams['ytick.labelsize']=14 # y轴标签数字大小

#无plt.rcParams['title.fontsize'] plt.title('x',fontsize=)中设置
plt.rcParams['legend.fontsize']=14 #图例字体大小
# 图大小
#定义一个画布
plt.figure()
'''
plt.figure(num=5,figsize=(4,4))
num指定figure画布的编号，figsize指定画布的大小
'''
#一个画布上有多个子图,两种方式
#第一种 subplots,一次创建多个子图，fig是画布对象，ax是子图对象array，当row_num>1 and col_num>1时，ax是个二维array，ax[0]是第一行画布
fig,ax=plt.subplots(row_num/2,col_num/3,figsize=(20,10))
ax=fig.subplots(row_num,col_num,figsize=(20,10))
'''
ax[1][2]=ax[1,2]
ax2=ax[1,2]
ax2.hist()#第二行第三个子图使用直方图
ax2.set_xlabel('设置子图x轴名字')
ax2.set_ylabel('设置子图y轴名字')
ax2.set_title('设置子图标题')
'''
#第二种subplot,当在其中一个子图中画sns图时，使用一次创建一个子图，画布从左到右从上到下的第3个
plt.subplot(2,3,3)==plt.subplot(233)
'''
如果想要设置画布大小的话
plt.figure(figsize=(x,y))
plt.subplot(122)
plt.plot()
#设置坐标轴名称
plt.xlabel('x label')
plt.ylabel('y label')
plt.title('title')

# sns配套使用
plt.subplot(122)
sns.violinplot(x='列名1',y='列名2',data=df)
'''
#设置坐标轴范围
plt.xlim((0,11))
plt.ylim((0.85,1))
plt.rcParams['axes.unicode_minus']=False # 用来正常显示负号
#设置连续数字的坐标轴精度
xticks=list(range(0,11,1))#[0,1,.,10]
plt.xticks(xticks)
#设置不连续数字，或者文字的坐标轴
xticks=['cws',1,'pos',2]
plt.xticks(list(range(len(xticks))),xticks)
#设置坐标轴名称,和字体大小
plt.rcParams['font.sans-serif']=['SimHei']#正常显示中文
plt.xlabel('x label'fontsize=)
plt.ylabel('y label'fontsize=)
plt.title('title', fontsize=)
#显示图
plt.show()
#保存画布
plt.savefig('路径')
```

* jupyter notebook

  ```python
  %matplotlib inline#添加该行
  ```


折线图

```python
import matplotlib.pyplot as plt
x=[]#表x坐标轴
labels=[]#每个颜色的折线都代表什么
colors=['red', 'blue', 'black', 'green']#多条折线颜色不同
X=[[],[],[]]#多维数组，一个[]是一根线的数据
for i in range(len(X)):
	plt.plot(x, X[i], c=colors[i], label=labels[i],marker='.')
	#markers=['.','v','>','<','^','o','*']等 标记每个点
    #没有color的话，也会默认给多条线分别不同的颜色
    #label是线的名字


plt.plot(x,y,'bo',label='xx')#'bo'为蓝色圆点，'b'为蓝色实线,
#设置坐标轴范围
plt.xlim(0,11)
plt.ylim(0.85,1)
#设置坐标轴精度
xticks=list(range(0,11,1))#[0,1,.,10]
plt.xticks(xticks)
#设置坐标轴名称
plt.xlabel=('x label')
plt.ylabel=('y label')
plt.title('title')

plt.legend(loc='best',fontsize=)##自适应方式放置多个图例（eg:每个颜色的折线都代表啥意思），默认放在  fontsize图例字体大小
'''
'best'         : 0,
'upper right'  : 1,
'upper left'   : 2,
'lower left'   : 3,
'lower right'  : 4,
'right'        : 5,
'center left'  : 6,
'center right' : 7,
'lower center' : 8,
'upper center' : 9,
'center'       : 10,
'''
#显示图
plt.show()
```

散点图

```python
plt.scatter(array_like_1,array_like_2,alpha=1,c='blue',marker='.')
'''
alpha为透明度，为0.6，颜色浅一些，更好看
c：散点图颜色
marker：散点图形状 #markers=['.','v','>','<','^','o','*']等 标记每个点
'''
plt.show()

不能直接df[['x','y']].scatter()
```

直方图 hist

```python
plt.hist(array_like/Series,bin=20,color='darkorange')#划分多少个区间

se.hist(bins=100)
se.plot('hist',bins=100)
sns.distplot(se) 带有外轮廓线
```

显示特征的分布情况（数字型为直方图，非数字型为条形图）

```python
distribution(df1, graph_num=10, graph_num_per_row=5)
# 特征的分布图（直方图or条形图）
def distribution(df, graph_num, graph_num_per_row):
    '''
    只使用取值在2~49的特征，特征最多使用graph_num个
    
    df：读取的文件
    nGraphShown：最多画几个特征的分布图
    nGraphPerRow：每行画几个图
    '''
    #Series，index为特征，值为该特征有几个取值
    nunique = df.nunique()
    #获取取值个数为2~49的所有特征的df
    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values
    #nRow样本数，ncol特征数
    nRow, nCol = df.shape
   # 取值个数为2~49的所有特征名的list
    columnNames = list(df)
    
    #要画几个特征的分布图
    real_cols=min(nCol,graph_num)
    #int(4.6)->4 几行图  10+5-1/5=2.8
    nGraphRow = int( (real_cols + graph_num_per_row - 1) / graph_num_per_row )
  
    plt.figure(figsize = (6 * graph_num_per_row, 6 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')
    for i in range(real_cols):
        plt.subplot(nGraphRow, graph_num_per_row, i + 1)
        columnDf = df.iloc[:, i]#是个Series
        if (not np.issubdtype(type(columnDf[0]), np.number)):#非数字型
            #每个取值各有几个，返回Series
            valueCounts = columnDf.value_counts()
            valueCounts.plot.bar()#条形图
        else: #是数字型号
            columnDf.hist()#直方图
        plt.ylabel('counts')
        plt.xticks(rotation = 90)
        plt.title(f'{columnNames[i]} (column {i})')
    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)
    plt.show()

```

显示特征的相关性

```python
简单的：
plt.figure(figsize=(18,9))
corr=df.corr()#特征已经挑出来了
sns.heatmap(corr)
plt.show()

#复杂点的
corr(df1, fig_size=(8,8),file_name='structures_bond.csv')

# Correlation matrix
#相关性图
def corr(df, fig_size, file_name):
    '''
    使用取值个数>1的特征
    
    graph_width：该图的宽度
    '''
    #去掉存在缺失值Nan的列
    df = df.dropna(axis=0, how='any') 
    
    #保留取值超过1个的特征
    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
    if df.shape[1] < 2:#若取值超过一个的特征只有一个
        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')
        return #返回None
    corr = df.corr()
    plt.figure( figsize=fig_size, dpi=80, facecolor='w', edgecolor='k')
    corrMat = plt.matshow(corr, fignum = 1)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)
    plt.gca().xaxis.tick_bottom()
    plt.colorbar(corrMat)
    plt.title(f'Correlation Matrix for {file_name}', fontsize=15)
    plt.show()
```

特征的散点图和密度图（对角线为特征的密度图【可选直方图】，其他为两个特征的散点图）

```python
scatter_density(df1, fig_size=(20,20), text_size=10)

# 散点图和密度图
def scatter_density(df, fig_size, text_size):
    '''
    最多使用9个特征
    '''
    #保留数字型
    df = df.select_dtypes(include =[np.number]) # keep only numerical columns
   
    df = df.dropna(axis=1, how='any')
    #保留取值超过1个的特征
    df = df[[col for col in df if df[col].nunique() > 1]] 
    columnNames = list(df)
    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots
        #最多保留9个特征
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=fig_size, diagonal='kde')
    '''
    alpha为透明度
    diagonal：对角线是 直方图'hist‘，还是核密度图 'kde'
    hist_kwds：传递给hist函数 {'bins':20}
    '''
    corrs = df.corr().values
    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=text_size)
    plt.suptitle('Scatter and Density Plot')
    plt.show()

```

不同类别的target的分布情况

```python
unique = train_df.type.unique()
# x轴标签大小
plt.rcParams['xtick.labelsize']=20
# y轴标签大小
plt.rcParams['ytick.labelsize']=20
print('类别数量：',len(unique))#2*4==8个类别
print('** 不同类别的分布 **')
plt.figure(figsize=(8*4,8*2))
for i in range(len(unique)):
    plt.subplot(2,4,i+1)
    df[df.type==unique[i]].target.hist(color='darkorange')
    plt.title(unique[i],fontsize=20)#标题字体大小
plt.show()

#若某个类别在取值上有明显的断层，可以在另一个特征的帮助下，将其再分子类

```



### How 解决数据不均衡？ ###

0. 数据不均衡：<code>一个类别大样本，一个类别小样本</code>

> 当模型的效果特别好时，有可能是样本不均衡：100个正常，1个坏的，都预测成好的，正确率99%

1. 重采样

   * ==欠==采样（==下==采样）：减少大样本类别的数量，抽取

     * ```
       类别越不均衡，欠采样抛弃的数据越多，可能抛弃了一些潜在的有用的信息，可能会由于数据量太少或依赖了不好的特征，训练了一个弱分类器
       ```

       

   * ==过==采样（==上==采样）：增加小样本类别的数量
     * 重复，或自举【（bootstrap method）有放回的均匀抽样】（可能会过拟合）
     * 生成新样本，SMOTE
       * 对于少数类样本a, 随机选择一个最近邻的样本b, 然后从a与b的连线上随机选取一个点c作为新的少数类样本

   * 交叉验证时采取过采样时要注意：（交叉验证时采取欠采样没有过拟合的担忧）

     ```
     不能在交叉验证之前过采样
     eg： k-fold，大样本数量=2*小样本数量
     将数据分成6份，大样本类别4份，小样本类别2份。每次实验，先选取验证集（6选1），然后对小样本进行过采样（非验证集的小样本），使得验证集+测试集=2x测试集
     ```

     * 当在交叉验证前采取过采样时，由于验证集和训练集有重复的数据，导致出现过拟合的结果

       ![1559963823235](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559963823235.png)

       * 正确做法：先交叉，分成k折，再过采样

         ![1559963904775](C:\Users\x1c\AppData\Roaming\Typora\typora-user-images\1559963904775.png)
         
         ```python
         from imblearn.over_sampling import SMOTE
         
         model=SMOTE(ratio={0:300,1:500},random_state=42)
         '''
         ratio：规定最终的样本数，0类别的样本数为300，1类别的样本数为500
         random_state: 随机种子
         '''
         model.fit_sample(X,y)
         '''
         X：为2维数据
         y: 类别至少为2
         '''
         ```
         
         

​		

​    

