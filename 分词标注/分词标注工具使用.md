# 分词标注工具

#### boson使用

**导包**

```python
from bosonnlp import BosonNLP
nlp = BosonNLP('p0UOLvIS.33617._QgKxWLWrf2p')#里面是ap，注册时给的
```

**分词**

```python
# 分词 将列表中的东西以‘ ’的形式连接起来，返回一个字符串,
format_result_tag = lambda tagged:' '.join('%s'%x for x in tagged['word'])
final=''
# 要加上encoding='utf-8'，否则会默认以GBK方式解码，出现错误
with open('raw_2.txt','r', encoding='utf-8') as f:
    for line in f:#取一行，带'\n'
        res = nlp.tag(line)
        #res=[{'word':[], 'tag':[]}]
        #res_txt 'ci_1 ci_2 ci_3'
        res_txt =format_result_tag(res[0])
        # 对该行（无'\n'）分词后，加上'\n'，使其还是一行
        final += res_txt + '\n'
with open('raw_2_tag.txt','w',encoding='utf-8') as ff:
    ff.write(final)          
```

**分词+标注**

```python
# 分词带标注，每段的标号不进行标注词性
format_result_tag_2 = lambda tagged:' '.join('%s/%s'%x for x in zip(tagged['word'][1:],tagged['tag'][1:]))
fin=''
# 要加上encoding='utf-8'，否则会默认以GBK方式解码，出现错误
with open('raw_2.txt','r', encoding='utf-8') as f:
    for line in f:#取一行，带'\n'
        res = nlp.tag(line)
        #res=[{'word':[], 'tag':[]}]
        #res_txt 'ci_1/v ci_2/n ci_3/a'
        res_txt =format_result_tag_2(res[0])
        # 对该行（无'\n'）分词后，加上'\n'，使其还是一行
        fin += res[0]['word'][0]+' '+res_txt+'\n'
with open('raw_2_tag.txt','w',encoding='utf-8') as ff:
    ff.write(fin)
```



#### jieba使用：最大字符串匹配+HMM

> 分词后，名字是一个整体

>    1）支持三种分词模式：
>
> 精确模式：将句子最精确的分开，适合文本分析
> 全模式：句子中所有可以成词的词语都扫描出来，速度快，不能解决歧义
> 搜索引擎模式：在精确的基础上，对长词再次切分，提高召回
>    2）支持繁体分词
>
>    3）支持自定义词典
>
>    4）基于 Trie 树结构实现高效的词图扫描，生成句子汉字所有可能成词情况所构成的有向无环图（DAG）
>
>    5)  采用了动态规划查找最大概率路径，找出基于词频的最大切分组合
>
>    6）对于词库中不存在的词，也就是未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法
>
> ![1556872589330](C:\Users\liangxiaochong\AppData\Roaming\Typora\typora-user-images\1556872589330.png)

**jieba使用**

```python
import jieba
# 分词 返回可迭代的generator。三种模式：全局（cut_all=True)，精确（默认:cut_all=False），搜索引擎模式

str='我来到北京清华大学'
#精确模式分词
words=jieba.cut(str, cut_all=False)# 我/ 来到/ 北京/ 清华大学
#全局模式分词
words=jieba.cut(str, cut_all=True)#我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
#搜索引擎模式:在精确模式基础上，对长词再进行切分，适合搜索引擎
words=jieba.cut_for_search()

# 分词+标注
import jieba.posseg as pseg
seg=pseg.cut(string)
for word, tag in seg:
	print(‘%s/%s’, %(word,tag))

#添加自定义词
jieba.add_word('石墨烯')
#添加自定义词典
jieba.load_userdict('userdict.txt')

#删除自定义词
jieba.del_word('自定义词')
```

